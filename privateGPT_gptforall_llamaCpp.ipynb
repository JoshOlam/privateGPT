{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.llms.gpt4all import GPT4All\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer: Let's think step by step.\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "local_path = (\n",
        "    \"./models/ggml-gpt4all-l13b-snoozy.bin\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from gpt4all import GPT4All\n",
        "# model = GPT4All(\"orca-mini-3b.ggmlv3.q4_0.bin\")\n",
        "# output = model.generate(\"The capital of France is \", max_tokens=3)\n",
        "# print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found model file at  /Users/josh/.cache/gpt4all/ggml-gpt4all-l13b-snoozy.bin\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Invalid model file\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Unable to instantiate model",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgpt4all\u001b[39;00m \u001b[39mimport\u001b[39;00m GPT4All\n\u001b[0;32m----> 2\u001b[0m model \u001b[39m=\u001b[39m GPT4All(\u001b[39m\"\u001b[39;49m\u001b[39mggml-gpt4all-l13b-snoozy.bin\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      3\u001b[0m output \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mgenerate(\u001b[39m\"\u001b[39m\u001b[39mThe capital of France is \u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[39mprint\u001b[39m(output)\n",
            "File \u001b[0;32m~/Documents/GitHub/privateGPT/pGPT_env/lib/python3.11/site-packages/gpt4all/gpt4all.py:98\u001b[0m, in \u001b[0;36mGPT4All.__init__\u001b[0;34m(self, model_name, model_path, model_type, allow_download, n_threads, device)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[39mif\u001b[39;00m device \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     97\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39minit_gpu(model_path\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig[\u001b[39m\"\u001b[39m\u001b[39mpath\u001b[39m\u001b[39m\"\u001b[39m], device\u001b[39m=\u001b[39mdevice)\n\u001b[0;32m---> 98\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mload_model(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mpath\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m     99\u001b[0m \u001b[39m# Set n_threads\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[39mif\u001b[39;00m n_threads \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
            "File \u001b[0;32m~/Documents/GitHub/privateGPT/pGPT_env/lib/python3.11/site-packages/gpt4all/pyllmodel.py:267\u001b[0m, in \u001b[0;36mLLModel.load_model\u001b[0;34m(self, model_path)\u001b[0m\n\u001b[1;32m    265\u001b[0m     llmodel\u001b[39m.\u001b[39mllmodel_loadModel(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, model_path_enc)\n\u001b[1;32m    266\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 267\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mUnable to instantiate model\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    269\u001b[0m filename \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mbasename(model_path)\n\u001b[1;32m    270\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_name \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39msplitext(filename)[\u001b[39m0\u001b[39m]\n",
            "\u001b[0;31mValueError\u001b[0m: Unable to instantiate model"
          ]
        }
      ],
      "source": [
        "from gpt4all import GPT4All\n",
        "model = GPT4All(\"ggml-gpt4all-l13b-snoozy.bin\")\n",
        "output = model.generate(\"The capital of France is \")\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "GPT4All.__init__() got an unexpected keyword argument 'model'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[8], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m callbacks \u001b[39m=\u001b[39m [StreamingStdOutCallbackHandler()]\n\u001b[1;32m      7\u001b[0m \u001b[39m# Verbose is required to pass to the callback manager\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m llm \u001b[39m=\u001b[39m GPT4All(model\u001b[39m=\u001b[39;49mlocal_path, callbacks\u001b[39m=\u001b[39;49mcallbacks, verbose\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     10\u001b[0m \u001b[39m# If you want to use a custom model add the backend parameter\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[39m# Check https://docs.gpt4all.io/gpt4all_python.html for supported backends\u001b[39;00m\n\u001b[1;32m     12\u001b[0m llm \u001b[39m=\u001b[39m GPT4All(model\u001b[39m=\u001b[39mlocal_path, backend\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgptj\u001b[39m\u001b[39m\"\u001b[39m, callbacks\u001b[39m=\u001b[39mcallbacks, verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
            "\u001b[0;31mTypeError\u001b[0m: GPT4All.__init__() got an unexpected keyword argument 'model'"
          ]
        }
      ],
      "source": [
        "local_path = (\n",
        "    \"./models/ggml-gpt4all-l13b-snoozy.bin\"\n",
        ")\n",
        "# Callbacks support token-wise streaming\n",
        "callbacks = [StreamingStdOutCallbackHandler()]\n",
        "\n",
        "# Verbose is required to pass to the callback manager\n",
        "llm = GPT4All(model=local_path, callbacks=callbacks, verbose=True)\n",
        "\n",
        "# If you want to use a custom model add the backend parameter\n",
        "# Check https://docs.gpt4all.io/gpt4all_python.html for supported backends\n",
        "llm = GPT4All(model=local_path, backend=\"gptj\", callbacks=callbacks, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'llm' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m llm_chain \u001b[39m=\u001b[39m LLMChain(prompt\u001b[39m=\u001b[39mprompt, llm\u001b[39m=\u001b[39mllm)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'llm' is not defined"
          ]
        }
      ],
      "source": [
        "llm_chain = LLMChain(prompt=prompt, llm=llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "question = \"What NFL team won the Super Bowl in the year Justin Bieber was born?\"\n",
        "\n",
        "llm_chain.run(question)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PcskHI-Xf2s"
      },
      "source": [
        "# Private GPT project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_bm7vKsPXXWF",
        "outputId": "817364de-7ae1-48d3-9ed7-83b2b6fb521e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'privateGPT'...\n",
            "remote: Enumerating objects: 361, done.\u001b[K\n",
            "remote: Counting objects: 100% (36/36), done.\u001b[K\n",
            "remote: Compressing objects: 100% (31/31), done.\u001b[K\n",
            "remote: Total 361 (delta 13), reused 16 (delta 5), pack-reused 325\u001b[K\n",
            "Receiving objects: 100% (361/361), 216.11 KiB | 6.00 MiB/s, done.\n",
            "Resolving deltas: 100% (185/185), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/imartinez/privateGPT.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2wxDEsfYXp5t",
        "outputId": "5a039ff9-70e8-4c9e-fb33-c1a250b004de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain==0.0.274 (from -r /content/privateGPT/requirements.txt (line 1))\n",
            "  Downloading langchain-0.0.274-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gpt4all==1.0.8 (from -r /content/privateGPT/requirements.txt (line 2))\n",
            "  Downloading gpt4all-1.0.8-py3-none-manylinux1_x86_64.whl (4.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting chromadb==0.4.7 (from -r /content/privateGPT/requirements.txt (line 3))\n",
            "  Downloading chromadb-0.4.7-py3-none-any.whl (415 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m415.5/415.5 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting llama-cpp-python==0.1.81 (from -r /content/privateGPT/requirements.txt (line 4))\n",
            "  Downloading llama_cpp_python-0.1.81.tar.gz (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: urllib3==2.0.4 in /usr/local/lib/python3.10/dist-packages (from -r /content/privateGPT/requirements.txt (line 5)) (2.0.4)\n",
            "Collecting PyMuPDF==1.23.1 (from -r /content/privateGPT/requirements.txt (line 6))\n",
            "  Downloading PyMuPDF-1.23.1-cp310-none-manylinux2014_x86_64.whl (4.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv==1.0.0 (from -r /content/privateGPT/requirements.txt (line 7))\n",
            "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Collecting unstructured==0.10.8 (from -r /content/privateGPT/requirements.txt (line 8))\n",
            "  Downloading unstructured-0.10.8-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting extract-msg==0.45.0 (from -r /content/privateGPT/requirements.txt (line 9))\n",
            "  Downloading extract_msg-0.45.0-py2.py3-none-any.whl (303 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m303.4/303.4 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tabulate==0.9.0 in /usr/local/lib/python3.10/dist-packages (from -r /content/privateGPT/requirements.txt (line 10)) (0.9.0)\n",
            "Collecting pandoc==2.3 (from -r /content/privateGPT/requirements.txt (line 11))\n",
            "  Downloading pandoc-2.3.tar.gz (33 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pypandoc==1.11 (from -r /content/privateGPT/requirements.txt (line 12))\n",
            "  Downloading pypandoc-1.11-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: tqdm==4.66.1 in /usr/local/lib/python3.10/dist-packages (from -r /content/privateGPT/requirements.txt (line 13)) (4.66.1)\n",
            "Collecting sentence_transformers==2.2.2 (from -r /content/privateGPT/requirements.txt (line 14))\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.274->-r /content/privateGPT/requirements.txt (line 1)) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.274->-r /content/privateGPT/requirements.txt (line 1)) (2.0.20)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.274->-r /content/privateGPT/requirements.txt (line 1)) (3.8.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.274->-r /content/privateGPT/requirements.txt (line 1)) (4.0.3)\n",
            "Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain==0.0.274->-r /content/privateGPT/requirements.txt (line 1))\n",
            "  Downloading dataclasses_json-0.5.14-py3-none-any.whl (26 kB)\n",
            "Collecting langsmith<0.1.0,>=0.0.21 (from langchain==0.0.274->-r /content/privateGPT/requirements.txt (line 1))\n",
            "  Downloading langsmith-0.0.28-py3-none-any.whl (34 kB)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.274->-r /content/privateGPT/requirements.txt (line 1)) (2.8.5)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.274->-r /content/privateGPT/requirements.txt (line 1)) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.274->-r /content/privateGPT/requirements.txt (line 1)) (2.2.1)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.274->-r /content/privateGPT/requirements.txt (line 1)) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.274->-r /content/privateGPT/requirements.txt (line 1)) (8.2.3)\n",
            "Collecting pydantic<3,>=1 (from langchain==0.0.274->-r /content/privateGPT/requirements.txt (line 1))\n",
            "  Downloading pydantic-1.10.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting chroma-hnswlib==0.7.2 (from chromadb==0.4.7->-r /content/privateGPT/requirements.txt (line 3))\n",
            "  Downloading chroma-hnswlib-0.7.2.tar.gz (31 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting fastapi<0.100.0,>=0.95.2 (from chromadb==0.4.7->-r /content/privateGPT/requirements.txt (line 3))\n",
            "  Downloading fastapi-0.99.1-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvicorn[standard]>=0.18.3 (from chromadb==0.4.7->-r /content/privateGPT/requirements.txt (line 3))\n",
            "  Downloading uvicorn-0.23.2-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting posthog>=2.4.0 (from chromadb==0.4.7->-r /content/privateGPT/requirements.txt (line 3))\n",
            "  Downloading posthog-3.0.2-py2.py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb==0.4.7->-r /content/privateGPT/requirements.txt (line 3)) (4.7.1)\n",
            "Collecting pulsar-client>=3.1.0 (from chromadb==0.4.7->-r /content/privateGPT/requirements.txt (line 3))\n",
            "  Downloading pulsar_client-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnxruntime>=1.14.1 (from chromadb==0.4.7->-r /content/privateGPT/requirements.txt (line 3))\n",
            "  Downloading onnxruntime-1.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers>=0.13.2 (from chromadb==0.4.7->-r /content/privateGPT/requirements.txt (line 3))\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m70.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pypika>=0.48.9 (from chromadb==0.4.7->-r /content/privateGPT/requirements.txt (line 3))\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting overrides>=7.3.1 (from chromadb==0.4.7->-r /content/privateGPT/requirements.txt (line 3))\n",
            "  Downloading overrides-7.4.0-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb==0.4.7->-r /content/privateGPT/requirements.txt (line 3)) (6.0.1)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb==0.4.7->-r /content/privateGPT/requirements.txt (line 3))\n",
            "  Downloading bcrypt-4.0.1-cp36-abi3-manylinux_2_28_x86_64.whl (593 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m593.7/593.7 kB\u001b[0m \u001b[31m47.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting diskcache>=5.6.1 (from llama-cpp-python==0.1.81->-r /content/privateGPT/requirements.txt (line 4))\n",
            "  Downloading diskcache-5.6.1-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.6/45.6 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting PyMuPDFb==1.23.0 (from PyMuPDF==1.23.1->-r /content/privateGPT/requirements.txt (line 6))\n",
            "  Downloading PyMuPDFb-1.23.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (26.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.3/26.3 MB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from unstructured==0.10.8->-r /content/privateGPT/requirements.txt (line 8)) (5.2.0)\n",
            "Collecting filetype (from unstructured==0.10.8->-r /content/privateGPT/requirements.txt (line 8))\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Collecting python-magic (from unstructured==0.10.8->-r /content/privateGPT/requirements.txt (line 8))\n",
            "  Downloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from unstructured==0.10.8->-r /content/privateGPT/requirements.txt (line 8)) (4.9.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from unstructured==0.10.8->-r /content/privateGPT/requirements.txt (line 8)) (3.8.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from unstructured==0.10.8->-r /content/privateGPT/requirements.txt (line 8)) (4.11.2)\n",
            "Collecting emoji (from unstructured==0.10.8->-r /content/privateGPT/requirements.txt (line 8))\n",
            "  Downloading emoji-2.8.0-py2.py3-none-any.whl (358 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m358.9/358.9 kB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting imapclient<3,>=2.3.0 (from extract-msg==0.45.0->-r /content/privateGPT/requirements.txt (line 9))\n",
            "  Downloading IMAPClient-2.3.1-py2.py3-none-any.whl (181 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.3/181.3 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting olefile==0.46 (from extract-msg==0.45.0->-r /content/privateGPT/requirements.txt (line 9))\n",
            "  Downloading olefile-0.46.zip (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.2/112.2 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tzlocal<6,>=4.2 in /usr/local/lib/python3.10/dist-packages (from extract-msg==0.45.0->-r /content/privateGPT/requirements.txt (line 9)) (5.0.1)\n",
            "Collecting compressed-rtf<2,>=1.0.6 (from extract-msg==0.45.0->-r /content/privateGPT/requirements.txt (line 9))\n",
            "  Downloading compressed_rtf-1.0.6.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ebcdic<2,>=1.1.1 (from extract-msg==0.45.0->-r /content/privateGPT/requirements.txt (line 9))\n",
            "  Downloading ebcdic-1.1.1-py2.py3-none-any.whl (128 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.5/128.5 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting RTFDE<0.2,>=0.1.0 (from extract-msg==0.45.0->-r /content/privateGPT/requirements.txt (line 9))\n",
            "  Downloading RTFDE-0.1.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.9/49.9 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting red-black-tree-mod==1.20 (from extract-msg==0.45.0->-r /content/privateGPT/requirements.txt (line 9))\n",
            "  Downloading red-black-tree-mod-1.20.tar.gz (28 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting plumbum (from pandoc==2.3->-r /content/privateGPT/requirements.txt (line 11))\n",
            "  Downloading plumbum-1.8.2-py3-none-any.whl (127 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.0/127.0 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ply (from pandoc==2.3->-r /content/privateGPT/requirements.txt (line 11))\n",
            "  Downloading ply-3.11-py2.py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers<5.0.0,>=4.6.0 (from sentence_transformers==2.2.2->-r /content/privateGPT/requirements.txt (line 14))\n",
            "  Downloading transformers-4.32.1-py3-none-any.whl (7.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m58.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers==2.2.2->-r /content/privateGPT/requirements.txt (line 14)) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence_transformers==2.2.2->-r /content/privateGPT/requirements.txt (line 14)) (0.15.2+cu118)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers==2.2.2->-r /content/privateGPT/requirements.txt (line 14)) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers==2.2.2->-r /content/privateGPT/requirements.txt (line 14)) (1.10.1)\n",
            "Collecting sentencepiece (from sentence_transformers==2.2.2->-r /content/privateGPT/requirements.txt (line 14))\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub>=0.4.0 (from sentence_transformers==2.2.2->-r /content/privateGPT/requirements.txt (line 14))\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.274->-r /content/privateGPT/requirements.txt (line 1)) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.274->-r /content/privateGPT/requirements.txt (line 1)) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.274->-r /content/privateGPT/requirements.txt (line 1)) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.274->-r /content/privateGPT/requirements.txt (line 1)) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.274->-r /content/privateGPT/requirements.txt (line 1)) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.274->-r /content/privateGPT/requirements.txt (line 1)) (1.3.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->unstructured==0.10.8->-r /content/privateGPT/requirements.txt (line 8)) (2.4.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.274->-r /content/privateGPT/requirements.txt (line 1))\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.274->-r /content/privateGPT/requirements.txt (line 1))\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting starlette<0.28.0,>=0.27.0 (from fastapi<0.100.0,>=0.95.2->chromadb==0.4.7->-r /content/privateGPT/requirements.txt (line 3))\n",
            "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers==2.2.2->-r /content/privateGPT/requirements.txt (line 14)) (3.12.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers==2.2.2->-r /content/privateGPT/requirements.txt (line 14)) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers==2.2.2->-r /content/privateGPT/requirements.txt (line 14)) (23.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from imapclient<3,>=2.3.0->extract-msg==0.45.0->-r /content/privateGPT/requirements.txt (line 9)) (1.16.0)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb==0.4.7->-r /content/privateGPT/requirements.txt (line 3))\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb==0.4.7->-r /content/privateGPT/requirements.txt (line 3)) (23.5.26)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb==0.4.7->-r /content/privateGPT/requirements.txt (line 3)) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb==0.4.7->-r /content/privateGPT/requirements.txt (line 3)) (1.12)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb==0.4.7->-r /content/privateGPT/requirements.txt (line 3))\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb==0.4.7->-r /content/privateGPT/requirements.txt (line 3))\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: python-dateutil>2.1 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb==0.4.7->-r /content/privateGPT/requirements.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from pulsar-client>=3.1.0->chromadb==0.4.7->-r /content/privateGPT/requirements.txt (line 3)) (2023.7.22)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.274->-r /content/privateGPT/requirements.txt (line 1)) (3.4)\n",
            "Collecting lark==1.1.5 (from RTFDE<0.2,>=0.1.0->extract-msg==0.45.0->-r /content/privateGPT/requirements.txt (line 9))\n",
            "  Downloading lark-1.1.5-py3-none-any.whl (107 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.0/108.0 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting oletools>=0.56 (from RTFDE<0.2,>=0.1.0->extract-msg==0.45.0->-r /content/privateGPT/requirements.txt (line 9))\n",
            "  Downloading oletools-0.60.1-py2.py3-none-any.whl (977 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m977.2/977.2 kB\u001b[0m \u001b[31m56.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.274->-r /content/privateGPT/requirements.txt (line 1)) (2.0.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers==2.2.2->-r /content/privateGPT/requirements.txt (line 14)) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers==2.2.2->-r /content/privateGPT/requirements.txt (line 14)) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers==2.2.2->-r /content/privateGPT/requirements.txt (line 14)) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence_transformers==2.2.2->-r /content/privateGPT/requirements.txt (line 14)) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence_transformers==2.2.2->-r /content/privateGPT/requirements.txt (line 14)) (16.0.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers==2.2.2->-r /content/privateGPT/requirements.txt (line 14)) (2023.6.3)\n",
            "Collecting safetensors>=0.3.1 (from transformers<5.0.0,>=4.6.0->sentence_transformers==2.2.2->-r /content/privateGPT/requirements.txt (line 14))\n",
            "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.7->-r /content/privateGPT/requirements.txt (line 3)) (8.1.7)\n",
            "Collecting h11>=0.8 (from uvicorn[standard]>=0.18.3->chromadb==0.4.7->-r /content/privateGPT/requirements.txt (line 3))\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb==0.4.7->-r /content/privateGPT/requirements.txt (line 3))\n",
            "  Downloading httptools-0.6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (428 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m428.8/428.8 kB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb==0.4.7->-r /content/privateGPT/requirements.txt (line 3))\n",
            "  Downloading uvloop-0.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m76.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb==0.4.7->-r /content/privateGPT/requirements.txt (line 3))\n",
            "  Downloading watchfiles-0.20.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m55.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb==0.4.7->-r /content/privateGPT/requirements.txt (line 3))\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured==0.10.8->-r /content/privateGPT/requirements.txt (line 8)) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers==2.2.2->-r /content/privateGPT/requirements.txt (line 14)) (3.2.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence_transformers==2.2.2->-r /content/privateGPT/requirements.txt (line 14)) (9.4.0)\n",
            "Collecting pyparsing<3,>=2.1.0 (from oletools>=0.56->RTFDE<0.2,>=0.1.0->extract-msg==0.45.0->-r /content/privateGPT/requirements.txt (line 9))\n",
            "  Downloading pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.8/67.8 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting easygui (from oletools>=0.56->RTFDE<0.2,>=0.1.0->extract-msg==0.45.0->-r /content/privateGPT/requirements.txt (line 9))\n",
            "  Downloading easygui-0.98.3-py2.py3-none-any.whl (92 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.7/92.7 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting colorclass (from oletools>=0.56->RTFDE<0.2,>=0.1.0->extract-msg==0.45.0->-r /content/privateGPT/requirements.txt (line 9))\n",
            "  Downloading colorclass-2.2.2-py2.py3-none-any.whl (18 kB)\n",
            "Collecting pcodedmp>=1.2.5 (from oletools>=0.56->RTFDE<0.2,>=0.1.0->extract-msg==0.45.0->-r /content/privateGPT/requirements.txt (line 9))\n",
            "  Downloading pcodedmp-1.2.6-py2.py3-none-any.whl (30 kB)\n",
            "Collecting msoffcrypto-tool (from oletools>=0.56->RTFDE<0.2,>=0.1.0->extract-msg==0.45.0->-r /content/privateGPT/requirements.txt (line 9))\n",
            "  Downloading msoffcrypto_tool-5.1.1-py3-none-any.whl (34 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb==0.4.7->-r /content/privateGPT/requirements.txt (line 3)) (3.7.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.274->-r /content/privateGPT/requirements.txt (line 1))\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb==0.4.7->-r /content/privateGPT/requirements.txt (line 3))\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence_transformers==2.2.2->-r /content/privateGPT/requirements.txt (line 14)) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb==0.4.7->-r /content/privateGPT/requirements.txt (line 3)) (1.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb==0.4.7->-r /content/privateGPT/requirements.txt (line 3)) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb==0.4.7->-r /content/privateGPT/requirements.txt (line 3)) (1.1.3)\n",
            "Requirement already satisfied: cryptography>=35.0 in /usr/local/lib/python3.10/dist-packages (from msoffcrypto-tool->oletools>=0.56->RTFDE<0.2,>=0.1.0->extract-msg==0.45.0->-r /content/privateGPT/requirements.txt (line 9)) (41.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=35.0->msoffcrypto-tool->oletools>=0.56->RTFDE<0.2,>=0.1.0->extract-msg==0.45.0->-r /content/privateGPT/requirements.txt (line 9)) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=35.0->msoffcrypto-tool->oletools>=0.56->RTFDE<0.2,>=0.1.0->extract-msg==0.45.0->-r /content/privateGPT/requirements.txt (line 9)) (2.21)\n",
            "Building wheels for collected packages: llama-cpp-python, pandoc, sentence_transformers, chroma-hnswlib, olefile, red-black-tree-mod, compressed-rtf, pypika\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.1.81-cp310-cp310-linux_x86_64.whl size=411868 sha256=06fb842c234044950c9f558fcf70cccc5c2886cafc0b59ee21f1703771481c46\n",
            "  Stored in directory: /root/.cache/pip/wheels/99/27/c0/ccda486d8dc16ebb8c61d9d7959380eced2cb3279cf0145dfe\n",
            "  Building wheel for pandoc (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pandoc: filename=pandoc-2.3-py3-none-any.whl size=33261 sha256=3cb0a95451a9a5a780ce348b0c62acb0d3001bcb6554d7b5dadd96de9316394f\n",
            "  Stored in directory: /root/.cache/pip/wheels/76/27/c2/c26175310aadcb8741b77657a1bb49c50cc7d4cdbf9eee0005\n",
            "  Building wheel for sentence_transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence_transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125923 sha256=7ddd91b4d0f11dc35e0303497c86da5d08fdec7a5403948892aa47b6ea6af66a\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
            "  Building wheel for chroma-hnswlib (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for chroma-hnswlib: filename=chroma_hnswlib-0.7.2-cp310-cp310-linux_x86_64.whl size=2287475 sha256=ac06eb2b4e4afc7013a2a10869a49500798de00120c42687a5057d1f4a32961d\n",
            "  Stored in directory: /root/.cache/pip/wheels/11/2b/0d/ee457f6782f75315bb5828d5c2dc5639d471afbd44a830b9dc\n",
            "  Building wheel for olefile (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for olefile: filename=olefile-0.46-py2.py3-none-any.whl size=35417 sha256=664a6e800ed527b4f672e64c6956ea9c5c272da59c85abdf9f03a59b6ad38f07\n",
            "  Stored in directory: /root/.cache/pip/wheels/02/39/c0/9eb1f7a42b4b38f6f333b6314d4ed11c46f12a0f7b78194f0d\n",
            "  Building wheel for red-black-tree-mod (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for red-black-tree-mod: filename=red_black_tree_mod-1.20-py3-none-any.whl size=18622 sha256=912dffaa12dae8e44ee342ca175fb024e6c03ac5bd112608266c0f58f6c95922\n",
            "  Stored in directory: /root/.cache/pip/wheels/1e/89/a0/17d08e78a59e4e8f51a95fe52e19c6916450c143acc7bce4dd\n",
            "  Building wheel for compressed-rtf (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for compressed-rtf: filename=compressed_rtf-1.0.6-py3-none-any.whl size=6185 sha256=c4ddd363b607a69fbd69d42f57df24391a3e0d673e52ee8b5498be7ae7118ad9\n",
            "  Stored in directory: /root/.cache/pip/wheels/15/3e/48/e7d833ecc516c36f8966d310b1a6386db091a718f1ff3bf85c\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53723 sha256=c50ed195c3ff599a1cd5f597ff7d64ace9b795e4d94384903e6ae76401e90984\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n",
            "Successfully built llama-cpp-python pandoc sentence_transformers chroma-hnswlib olefile red-black-tree-mod compressed-rtf pypika\n",
            "Installing collected packages: tokenizers, sentencepiece, safetensors, red-black-tree-mod, pypika, ply, monotonic, lark, filetype, ebcdic, easygui, compressed-rtf, websockets, uvloop, python-magic, python-dotenv, pyparsing, pypandoc, PyMuPDFb, pydantic, pulsar-client, plumbum, overrides, olefile, mypy-extensions, marshmallow, imapclient, humanfriendly, httptools, h11, emoji, diskcache, colorclass, chroma-hnswlib, bcrypt, backoff, watchfiles, uvicorn, unstructured, typing-inspect, starlette, PyMuPDF, posthog, pandoc, llama-cpp-python, langsmith, huggingface-hub, gpt4all, coloredlogs, transformers, onnxruntime, msoffcrypto-tool, fastapi, dataclasses-json, langchain, chromadb, pcodedmp, oletools, RTFDE, sentence_transformers, extract-msg\n",
            "  Attempting uninstall: pyparsing\n",
            "    Found existing installation: pyparsing 3.1.1\n",
            "    Uninstalling pyparsing-3.1.1:\n",
            "      Successfully uninstalled pyparsing-3.1.1\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.2.1\n",
            "    Uninstalling pydantic-2.2.1:\n",
            "      Successfully uninstalled pydantic-2.2.1\n",
            "Successfully installed PyMuPDF-1.23.1 PyMuPDFb-1.23.0 RTFDE-0.1.0 backoff-2.2.1 bcrypt-4.0.1 chroma-hnswlib-0.7.2 chromadb-0.4.7 colorclass-2.2.2 coloredlogs-15.0.1 compressed-rtf-1.0.6 dataclasses-json-0.5.14 diskcache-5.6.1 easygui-0.98.3 ebcdic-1.1.1 emoji-2.8.0 extract-msg-0.45.0 fastapi-0.99.1 filetype-1.2.0 gpt4all-1.0.8 h11-0.14.0 httptools-0.6.0 huggingface-hub-0.16.4 humanfriendly-10.0 imapclient-2.3.1 langchain-0.0.274 langsmith-0.0.28 lark-1.1.5 llama-cpp-python-0.1.81 marshmallow-3.20.1 monotonic-1.6 msoffcrypto-tool-5.1.1 mypy-extensions-1.0.0 olefile-0.46 oletools-0.60.1 onnxruntime-1.15.1 overrides-7.4.0 pandoc-2.3 pcodedmp-1.2.6 plumbum-1.8.2 ply-3.11 posthog-3.0.2 pulsar-client-3.3.0 pydantic-1.10.12 pypandoc-1.11 pyparsing-2.4.7 pypika-0.48.9 python-dotenv-1.0.0 python-magic-0.4.27 red-black-tree-mod-1.20 safetensors-0.3.3 sentence_transformers-2.2.2 sentencepiece-0.1.99 starlette-0.27.0 tokenizers-0.13.3 transformers-4.32.1 typing-inspect-0.9.0 unstructured-0.10.8 uvicorn-0.23.2 uvloop-0.17.0 watchfiles-0.20.0 websockets-11.0.3\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pyparsing"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install -r /content/privateGPT/requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMXvANMFY7vI"
      },
      "source": [
        " Download the LLM model and place it in a directory of your choice:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HqMUhdheY2Ru",
        "outputId": "fb881cbf-0b1e-45b1-c4b7-7db628158812"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-08-30 10:33:43--  https://gpt4all.io/models/ggml-gpt4all-j-v1.3-groovy.bin\n",
            "Resolving gpt4all.io (gpt4all.io)... 104.26.0.159, 104.26.1.159, 172.67.71.169, ...\n",
            "Connecting to gpt4all.io (gpt4all.io)|104.26.0.159|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3785248281 (3.5G)\n",
            "Saving to: ‘ggml-gpt4all-j-v1.3-groovy.bin’\n",
            "\n",
            "ggml-gpt4all-j-v1.3 100%[===================>]   3.52G  64.4MB/s    in 66s     \n",
            "\n",
            "2023-08-30 10:34:49 (54.8 MB/s) - ‘ggml-gpt4all-j-v1.3-groovy.bin’ saved [3785248281/3785248281]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://gpt4all.io/models/ggml-gpt4all-j-v1.3-groovy.bin\n",
        "\n",
        "#copy path of download on colab files and edit the env model path with it\n",
        "#MODEL_PATH=models/ggml-gpt4all-j-v1.3-groovy.bin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q3PdaKUxaqx7"
      },
      "outputs": [],
      "source": [
        "#create the .env file by copying the contents in the example.env\n",
        "#update the model path\n",
        "\n",
        "#or edit the ingest.py to use example.env"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAq7vROGdfRH"
      },
      "source": [
        "In Google Colab, you might encounter issues when trying to name a file starting with a dot (e.g., .env). This is because files and directories starting with a dot are treated as hidden files in Unix-like systems, and Google Colab's file management system is based on Unix-like conventions.\n",
        "\n",
        "\n",
        "Create a Normal File Name: First, create the file with a different name, like `env.txt`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Nhtlqzu9dIAH"
      },
      "outputs": [],
      "source": [
        "!touch env.txt #move file into the PrivateGPT folder\n",
        "#then copy and paste the contents from example.env , env variables into it.\n",
        "\n",
        "#changes to the colab version;\n",
        "\n",
        "## --->where the file/dir path was updated to suite google colab\n",
        "\n",
        "##PERSIST_DIRECTORY=/content/db\n",
        "#MODEL_TYPE=GPT4All\n",
        "##MODEL_PATH=/content/ggml-gpt4all-j-v1.3-groovy.bin\n",
        "#EMBEDDINGS_MODEL_NAME=all-MiniLM-L6-v2\n",
        "#MODEL_N_CTX=1000\n",
        "#MODEL_N_BATCH=8\n",
        "#TARGET_SOURCE_CHUNKS=4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wK5CVZdZd2dW"
      },
      "source": [
        "Rename the File: Then, rename the file to `.env`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "bfjFk6bcd1ie"
      },
      "outputs": [],
      "source": [
        "# Rename the file to .env\n",
        "import os\n",
        "os.rename('/content/privateGPT/env.txt', '.env')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZvW6cBXTeiig",
        "outputId": "6e504840-e41d-4a14-e4d1-f843c8bd5b01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ".   .config  ggml-gpt4all-j-v1.3-groovy.bin  privateGPT\n",
            "..  .env     .ipynb_checkpoints\t\t     sample_data\n"
          ]
        }
      ],
      "source": [
        "#if the .env file doesnt show in the contents/files in your colab\n",
        "!ls -a\n",
        "#list files in dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3lfl6S3ep_t",
        "outputId": "9b44c929-6cf5-4f0c-a73f-c0c96f027007"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PERSIST_DIRECTORY=/content/db\r\n",
            "MODEL_TYPE=GPT4All\r\n",
            "MODEL_PATH=/content/ggml-gpt4all-j-v1.3-groovy.bin\r\n",
            "EMBEDDINGS_MODEL_NAME=all-MiniLM-L6-v2\r\n",
            "MODEL_N_CTX=1000\r\n",
            "MODEL_N_BATCH=8\r\n",
            "TARGET_SOURCE_CHUNKS=4"
          ]
        }
      ],
      "source": [
        "#if .env is there, use cat command to display content\n",
        "!cat .env\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmOaj_uZgJAw"
      },
      "source": [
        "check if the file has the right reading permissions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Mg8R0q1hgTa4"
      },
      "outputs": [],
      "source": [
        "!chmod +r .env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u6JFSaFxtblJ"
      },
      "outputs": [],
      "source": [
        "#make edits on the ingest.py file and privategpt.py file on the\n",
        "#dotenv.load_dotenv(\"/full/path/to/.env\")\n",
        "\n",
        "#also make updates to this\n",
        "#source_directory = os.environ.get('SOURCE_DIRECTORY', \"/content/privateGPT/source_documents\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Za4K23RprQMl"
      },
      "source": [
        "Ingest all data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-2zo74_aF-N",
        "outputId": "be92f710-dde6-496d-e6df-3fceb708e639"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\rDownloading (…)e9125/.gitattributes:   0% 0.00/1.18k [00:00<?, ?B/s]\rDownloading (…)e9125/.gitattributes: 100% 1.18k/1.18k [00:00<00:00, 3.88MB/s]\n",
            "Downloading (…)_Pooling/config.json: 100% 190/190 [00:00<00:00, 953kB/s]\n",
            "Downloading (…)7e55de9125/README.md: 100% 10.6k/10.6k [00:00<00:00, 27.9MB/s]\n",
            "Downloading (…)55de9125/config.json: 100% 612/612 [00:00<00:00, 2.18MB/s]\n",
            "Downloading (…)ce_transformers.json: 100% 116/116 [00:00<00:00, 439kB/s]\n",
            "Downloading (…)125/data_config.json: 100% 39.3k/39.3k [00:00<00:00, 9.70MB/s]\n",
            "Downloading pytorch_model.bin: 100% 90.9M/90.9M [00:00<00:00, 92.2MB/s]\n",
            "Downloading (…)nce_bert_config.json: 100% 53.0/53.0 [00:00<00:00, 216kB/s]\n",
            "Downloading (…)cial_tokens_map.json: 100% 112/112 [00:00<00:00, 341kB/s]\n",
            "Downloading (…)e9125/tokenizer.json: 100% 466k/466k [00:00<00:00, 7.92MB/s]\n",
            "Downloading (…)okenizer_config.json: 100% 350/350 [00:00<00:00, 1.34MB/s]\n",
            "Downloading (…)9125/train_script.py: 100% 13.2k/13.2k [00:00<00:00, 35.7MB/s]\n",
            "Downloading (…)7e55de9125/vocab.txt: 100% 232k/232k [00:00<00:00, 8.82MB/s]\n",
            "Downloading (…)5de9125/modules.json: 100% 349/349 [00:00<00:00, 1.22MB/s]\n",
            "Creating new vectorstore\n",
            "Loading documents from /content/privateGPT/source_documents\n",
            "Loading new documents: 100%|█████████████████████| 1/1 [00:00<00:00, 135.57it/s]\n",
            "Loaded 1 new documents from /content/privateGPT/source_documents\n",
            "Split into 91 chunks of text (max. 500 tokens each)\n",
            "Creating embeddings. May take some minutes...\n",
            "Ingestion complete! You can now run privateGPT.py to query your documents\n"
          ]
        }
      ],
      "source": [
        "!python /content/privateGPT/ingest.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKpRHLv6rSro"
      },
      "source": [
        "Ask questions to our documents locally"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_6wQiXtPrWMx",
        "outputId": "4398045e-0405-47af-ba7a-4e92a436c781"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found model file at  /content/ggml-gpt4all-j-v1.3-groovy.bin\n",
            "gptj_model_load: loading model from '/content/ggml-gpt4all-j-v1.3-groovy.bin' - please wait ...\n",
            "gptj_model_load: n_vocab = 50400\n",
            "gptj_model_load: n_ctx   = 2048\n",
            "gptj_model_load: n_embd  = 4096\n",
            "gptj_model_load: n_head  = 16\n",
            "gptj_model_load: n_layer = 28\n",
            "gptj_model_load: n_rot   = 64\n",
            "gptj_model_load: f16     = 2\n",
            "gptj_model_load: ggml ctx size = 5401.45 MB\n",
            "gptj_model_load: kv self size  =  896.00 MB\n",
            "gptj_model_load: ................................... done\n",
            "gptj_model_load: model size =  3609.38 MB / num tensors = 285\n",
            "\n",
            "Enter a query: state of the union is what?\n",
            " The State of the Union refers to a report given by the President or Vice President during an annual address delivered in Congress, outlining their vision for the country and its current political situation.\n",
            "\n",
            "> Question:\n",
            "state of the union is what?\n",
            "\n",
            "> Answer (took 2670.53 s.):\n",
            " The State of the Union refers to a report given by the President or Vice President during an annual address delivered in Congress, outlining their vision for the country and its current political situation.\n",
            "\n",
            "> /content/privateGPT/source_documents/state_of_the_union.txt:\n",
            "The only nation that can be defined by a single word: possibilities. \n",
            "\n",
            "So on this night, in our 245th year as a nation, I have come to report on the State of the Union. \n",
            "\n",
            "And my report is this: the State of the Union is strong—because you, the American people, are strong. \n",
            "\n",
            "We are stronger today than we were a year ago. \n",
            "\n",
            "And we will be stronger a year from now than we are today. \n",
            "\n",
            "Now is our moment to meet and overcome the challenges of our time. \n",
            "\n",
            "And we will, as one people. \n",
            "\n",
            "One America.\n",
            "\n",
            "> /content/privateGPT/source_documents/state_of_the_union.txt:\n",
            "Let’s increase Pell Grants and increase our historic support of HBCUs, and invest in what Jill—our First Lady who teaches full-time—calls America’s best-kept secret: community colleges. \n",
            "\n",
            "And let’s pass the PRO Act when a majority of workers want to form a union—they shouldn’t be stopped.  \n",
            "\n",
            "When we invest in our workers, when we build the economy from the bottom up and the middle out together, we can do something we haven’t done in a long time: build a better America.\n",
            "\n",
            "> /content/privateGPT/source_documents/state_of_the_union.txt:\n",
            "Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.  \n",
            "\n",
            "Last year COVID-19 kept us apart. This year we are finally together again. \n",
            "\n",
            "Tonight, we meet as Democrats Republicans and Independents. But most importantly as Americans. \n",
            "\n",
            "With a duty to one another to the American people to the Constitution. \n",
            "\n",
            "And with an unwavering resolve that freedom will always triumph over tyranny.\n",
            "\n",
            "> /content/privateGPT/source_documents/state_of_the_union.txt:\n",
            "Powered by people I’ve met like JoJo Burgess, from generations of union steelworkers from Pittsburgh, who’s here with us tonight. \n",
            "\n",
            "As Ohio Senator Sherrod Brown says, “It’s time to bury the label “Rust Belt.” \n",
            "\n",
            "It’s time. \n",
            "\n",
            "But with all the bright spots in our economy, record job growth and higher wages, too many families are struggling to keep up with the bills.  \n",
            "\n",
            "Inflation is robbing them of the gains they might otherwise feel.\n",
            "\n",
            "Enter a query: Traceback (most recent call last):\n",
            "  File \"/content/privateGPT/privateGPT.py\", line 89, in <module>\n",
            "    main()\n",
            "  File \"/content/privateGPT/privateGPT.py\", line 52, in main\n",
            "    query = input(\"\\nEnter a query: \")\n",
            "KeyboardInterrupt\n",
            "Exception ignored in atexit callback: <bound method Client.join of <posthog.client.Client object at 0x7817c2377af0>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/posthog/client.py\", line 400, in join\n",
            "    consumer.join()\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 1096, in join\n",
            "    self._wait_for_tstate_lock()\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 1116, in _wait_for_tstate_lock\n",
            "    if lock.acquire(block, timeout):\n",
            "KeyboardInterrupt: \n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!python /content/privateGPT/privateGPT.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4E4XKkOazMgk"
      },
      "source": [
        "Download a folder from colab\n",
        "\n",
        "Manual Download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "MNMIvqEn0gFo",
        "outputId": "93c895a4-36fa-4ae1-a215-3e42a555c245"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<a href='/content/privateGPT.zip' target='_blank'>/content/privateGPT.zip</a><br>"
            ],
            "text/plain": [
              "/content/privateGPT.zip"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import shutil\n",
        "from IPython.display import FileLink\n",
        "\n",
        "# Replace 'folder_to_download' with the actual folder name\n",
        "folder_name = '/content/privateGPT'\n",
        "\n",
        "# Zip the folder\n",
        "shutil.make_archive(folder_name, 'zip', folder_name)\n",
        "\n",
        "# Generate download link\n",
        "zip_filename = f'{folder_name}.zip'\n",
        "FileLink(zip_filename)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UUwsyQ-h1r2V"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJZ53Ay07J-C"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNrb91vUfRPgw5z4gza8RI+",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
